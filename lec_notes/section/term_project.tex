
\chapter{Visual-Inertial Odometry SLAM}

\section{Project Framework}
The goal of this term project is to reinforce everything that we have learned	 in class so far and some more. In particular, I would briefly give an overview of SLAM: the classical and the modern, walk you through some example simple applications and then you will be required to implement a visual inertial odometry SLAM navigation of your robot in an environment of your own choosing (that I will approve). I recommend that you take a look at the stereo-visual inertial odometry example on the \href{https://docs.nvidia.com/isaac/isaac/packages/perception/doc/visual_odometry.html}{isaac webpage} before you send me your proposals.  The Isaac SDK provides a precompiled version of the Elbrus visual odometry library to localize the robot in the world. It does this by continuously analyzing the information from the video stream (in this case the realsense camera sensor). Proposal submissions shall be due in earnest by 23:59:59 on Saturday March 14th, 2020.

\section{Simultaneous Localization and Mapping (SLAM)}

%Since we have a vision-based robot, we will be doing a visual (inertial) odometry-based simultaneous localization and mapping (SLAM). 
By SLAM, we mean the concurrent construction of an environment's model (the map) and the state estimation of the moving robot within it. The robot's state includes the positions, velocities and orientations of different components of the robot per time, although different components may be included such as the bias of the sensors and sensor calibration parameters. The map is used because it can inform of path planning or provide an intuitive visualization for a human operator. The map also allows us to limit the state estimation errors so that dead-reckoning does not drift over time. With the map, the robot is able to reset its localization error by revisiting known areas in a process called \textit{loop-closure}. Sometimes, \textit{a priori} set of landmarks are given to the robot to ease the process of map build-up. For instance, in a manufacturing warehouse where the location of objects and obstacles do not change much, a manually built map can be constructed based on the artificial beacons in the environment. If the robot has access to a GPS, such GPS satellites may be considered as moving beacons at known locations. If such localizations can be reliably carried out, SLAM may not be required\footnote{I promise you that SLAM will be required for the tasks in this term project though.}.  The estimation of the robot's  state  is typically carried out with standard filters such as the extended Kalman filter (ekf).  

SLAM has a common ground with research fields such as computer vision and signal processing. At a higher level, SLAM is a good admixture of geometry, graph theory, optimization, and probabilistic estimation. A SLAM developer has to deal with the practical aspects of integration ranging from sensor calibration, sensor fusion to system integration. It helps to separate SLAM by class into the classical age and the contemporary age. During the classical age, essentially 1986 - 2004, there was the introduction of probabilistic formulations for SLAM, including approaches such as extended Kalman filters, Rao-Blackwellized particle filters and maximum likelihood estimation. With these algorithms, there were the basic challenges connected to efficiency and robust data collection. As such people started looking into ways of conducting better localization and state estimation by merging inertial measurements with visual odometry. This has proven to be really effective in slowly-moving navigation tasks such as MERS robots. % For the remaining weeks in this class, we will need all our programming, mathematical and algorithmic skills in bringing to bear a good SLAM-algorithmic development on our robot.
 
 The goal of SLAM is to build a globally-consistent representation of the environment, leveraging ego-motion measurements and loop closures. Earlier SLAM applications involved odometry being found by integrating wheel encoders. However, the pose estimate from such wheel odometry drifts quickly making the estimate unreliable after a few meters. Recent odometry algorithms are however based on visual and inertial information, and these tend to have very small drift ($< 0.5\%$ of the trajectory length~\cite{SLAMLeonard}). 
 
 In the evolution of robot-based SLAM research, mapping a 2-D indoor environment with a robot that has wheel encoders and a laser scanner with enough accuracy ($< 10$cm) and enough robustness (\eg low failure rate) is almost a solved problem. You can look through the codes available on this page that implements this 2D indoor navigation using the Pioneer P3-DX robot \href{https://github.com/SeRViCE-Lab/p3-dx}{\textit{P3-DX}}. Feel free to play with the simulation using the ROSARIA and ROSARNL libraries as described in the README files on the page. A good industrial example is the \href{https://www.kuka.com/-/media/kuka-downloads/imported/9cb8e311bfd744b4b0eab25ca883f6d3/kuka_navigation_solution_en.pdf}{\textit{Kuka Navigation solution}}. Vision-based SLAM for slowly-moving robots such as the Mars exploration  rovers~\cite{MarsVisualOdometry} can be considered an almost solved problem as well. For example, prior to 2006, NASA's MER were commanded only once per Martian solar day using a prescheduled sequence of precise metrically specified commands such as ``drive forward 2.34 meters, turn in place $0.3567$ radians to the right, drive to location $X, Y$ and take colored pictures of the terrain at location $X,Y, Z$~\cite{leger2005mars}.

%\section{GMapping}
%To successfully be able to navigate a robot in an environment, the grid map of the robot's environment must be built from the ground up. GMapping is a widely used tool in robotics navigation to solve the grid world building and update process. It is based on the OpenSLAM \

\section{Relevant Texts and Materials}

\begin{itemize}
	\item SLAM on the Mars Exploration Rovers: \href{https://onlinelibrary.wiley.com/doi/pdf/10.1002/rob.20184}{Maimone, Mark, Yang Cheng, and Larry Matthies. "Two years of visual odometry on the mars exploration rovers." Journal of Field Robotics 24, no. 3 (2007): 169-186.}
	%
	\item Visual Inertial Localization: \href{http://www.roboticsproceedings.org/rss11/p37.pdf}{Lynen, Simon, Torsten Sattler, Michael Bosse, Joel A. Hesch, Marc Pollefeys, and Roland Siegwart. "Get out of my lab: Large-scale, real-time visual-inertial localization." In Robotics: Science and Systems, vol. 1. 2015.}
	%
	\item SLAM Survey: \href{https://ieeexplore-ieee-org.proxy.library.upenn.edu/stamp/stamp.jsp?tp=&arnumber=7747236}{Cadena, Cesar, Luca Carlone, Henry Carrillo, Yasir Latif, Davide Scaramuzza, JosÃ© Neira, Ian Reid, and John J. Leonard. "Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age." IEEE Transactions on robotics 32, no. 6 (2016): 1309-1332.}
	%
	\item SLAM Tutorial: Part I. \href{}{Durrant-Whyte, Hugh, and Tim Bailey. "Simultaneous localization and mapping: part I." IEEE robotics \& automation magazine 13, no. 2 (2006): 99-110.}
	%
	\item SLAM Tutorial: Part II. \href{https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1678144}{Bailey, Tim, and Hugh Durrant-Whyte. "Simultaneous localization and mapping (SLAM): Part II." IEEE robotics \& automation magazine 13, no. 3 (2006): 108-117.}
\end{itemize}