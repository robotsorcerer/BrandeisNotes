\chapter{Vectors and Matrices}  
 \label{chap:vectors::intro}
 
 In the previous chapter, we looked into the problem of the minima and maxima (locally) of a function of a single and two variables. Suppose that we have $N$ variables, and proceed in a similar manner as before, we see that finding basic necessary and sufficient conditions that ensure the positivity of a quadratic form of $N$ variables are of the form
 %
 \begin{align}
 	Q(x_1, x_2, \ldots, x_N) = \sum_{i,j=1}^{N} a_{ij} x_i x_j
 \end{align}
 %
 We will thus develop a notation that allows us to solve the problem \textit{analytically} using a minimum of arithmetic or analytical calculation. In this light, we will develop a notation that allows us to study linear transformations such as
 %
 \begin{align}
 	y_i = \sum_{j=1}^{N} a_{ij} x_j \qquad i=1,2, \ldots, N
 \end{align}
 
 \section{Vectors}
 
 We shall define a set of $N$ complex-valued numbers as a \textit{vector}, written as
 %
 \begin{align}
 	\bm{x} = \begin{bmatrix}
 	x_1 \\ x_2 \\ \vdots \\ x_N
 	\end{bmatrix}
 	\label{eq:col_vec}
 \end{align}
 %
 The vector $\bm{x}$ in \eqref{eq:col_vec} shall be called a \textit{column vector}. If the elements of the vector are stacked horizontally, \ie 
 %
 \begin{align}
\bm{x} = \begin{bmatrix}
x_1 & x_2 & \ldots & x_N,
\end{bmatrix}
\label{eq:row_vec}
\end{align}
%
then we shall call it a \textit{row vector}.

Going forward, we shall use the notation of \eqref{eq:col_vec} to represent all forms of vectors we shall be using. When we mean a row vector, we shall use the notation of a transpose of \eqref{eq:col_vec}, \ie $\bm{x}^T$. Bold font letters such as $\bm{x}$, or  $\bm{y}$ shall denote vectors and lower-case letters with subscripts $i$ such as $x_i, y_i, z_i$ or $p_i, q_i, r_i$ shall denote the components of a vector. When discussing a particular set of vectors, we shall use the superscripts $\bm{x}^1, \bm{x}^2$ e.t.c. $N$ shall denote the dimension of a vector $\bm{x}$.

One-dimensional vectors are called \textit{scalars} and shall be our quantities of analysis. When we write $\bar{\bm{x}}$, we shall mean the vector whose components are the complex conjugates of the elements of $\bm{x}$.

\subsection{Addition of Vectors}
%
Two vectors $\bm{x}$ and $\bm{y}$ are said to be equal if all of their components, $(x_i, y_i)$ are equal for $i=1,2,\ldots, N$. Addition is the simplest of the arithmetic operations on vectors. We shall write the sum of two vectors as $\bm{x} + \bm{y}$ so that 
%
\begin{align}
	\bm{x} + \bm{y} = \begin{bmatrix}
	x_1 + y_1 \\ x_2+y_2 \\ \vdots \\ x_N + y_N,
	\end{bmatrix}
\end{align}
% 
whereupon we note that the $``+"$ sign connecting $\bm{x}$ and $\bm{y}$ is different from the one connecting $x_i$ and $y_i$.

\begin{homework}
	Prove that we have the \textit{commutativity}, $\bm{x}+\bm{y} = \bm{y} + \bm{x}$, and the associativity $\bm{x} + \left(\bm{y}+\bm{z}\right) = \left(\bm{x} + \bm{y}\right)+\bm{z}$
\end{homework}
%
\begin{homework}
	Just as we showed the addition property of two vectors above, show the subtraction property of two vectors $\bm{x}$ and $\bm{y}$.
\end{homework}

\subsection{Scalar Multiplication}
When a vector is multiplied by a scalar, we shall write it out as follows
%
\begin{align}
	c_1 \bm{x} = \bm{x} c_1 = \begin{bmatrix}
	c_1 x_1 \\ c_1 x_2 \\ \vdots \\ c_1 x_N
	\end{bmatrix}
\end{align}
%
\subsection{The Inner Product of Two Vectors}
%
This is a scalar function of two vectors $\bm{x}$ and $\bm{y}$ defined as 
%
\begin{align}
	\langle \bm{x}, \bm{y} \rangle = \sum_{i=1}^{N} x_i y_i.
\end{align}

Further to the above, we define the following properties for inner product
%
\begin{subequations}
	\begin{align}
	\langle \bm{x}, \bm{y} \rangle &= 	\langle \bm{y}, \bm{x} \rangle \\
	\langle \bm{x} + \bm{y},  \bm{u} + \bm{v}\rangle &= \langle \bm{x}, \bm{u} \rangle  + \langle \bm{x}, \bm{v} \rangle + \langle \bm{y}, \bm{u} \rangle + \langle \bm{y}, \bm{v} \rangle \\
	\langle c_1 \bm{x}, \bm{y} \rangle &= c_1 \langle \bm{x}, \bm{y} \rangle
	\end{align}
\end{subequations}
%
The above is an easy way to \textit{multiply} two vectors. The inner product is important because $\langle \bm{x}, \bm{x} \rangle$ can be considered as the square of the ``length" of the real vector $\bm{x}$. 
%
\begin{homework}
	Prove that $\langle a\bm{x}+b \bm{y}, a\bm{x}+b \bm{y} \rangle = a^2 \langle \bm{x}, \bm{x} \rangle + 2 ab \langle \bm{x}, \bm{y} \rangle  + b^2 \langle \bm{y}, \bm{y} \rangle $ is a non-negative quadratic form in the scalar variables $a$ and $b$ if $\bm{x}$ and $\bm{y}$ are real.
\end{homework}
%
\begin{homework}
	Hence, show that for real-valued vectors $\bm{x}$ and $\bm{y}$, that the Cauchy-Schwarz Inequality $\langle \bm{x}, \bm{y} \rangle^2 \le  \langle \bm{x}, \bm{x} \rangle \langle \bm{y}, \bm{y} \rangle$ holds.
\end{homework}
%
\begin{homework}
	Using the above result, show that  for any two complex vectors $\bm{x}$  and $\bm{y}$, $|\langle \bm{x}, \bm{y}\rangle|^2 \le \langle \bm{x}, \bar{\bm{x}}\rangle \, \langle \bm{y}, \bar{\bm{y}}\rangle$
\end{homework}
%
\begin{homework}
	Show that the \textit{triangle inequality} 
	\[
		\langle \bm{x}+ \bm{y}, \bm{x}+ \bm{y} \rangle^{\frac{1}{2}} \le \langle \bm{x}, \bm{x} \rangle^{\frac{1}{2}}  + \langle  \bm{y},  \bm{y} \rangle^{\frac{1}{2}} 
	\]
	holds for any two real-valued variables.
\end{homework}

\subsection{Orthogonality}

Two vectors are said to be orthogonal if their inner product is $0$ \ie 
%
\begin{align}
	\langle \bm{x}, \bm{y}\rangle = 0
\end{align}
%
When the set of real vectors $\{\bm{x}^i\}$ possess the property that $\langle \bm{x}^i, \bm{y}^i\rangle = 1$, then we say they are \textit{orthonormal.}
%
\begin{homework}
	show that $\bm{x}^i$ are mutually orthogonal and normalized \ie orthonormal for the following $N$-dimensional Euclidean basis coordinate vectors
	\begin{align}
		\bm{x}^1 = \begin{bmatrix}
		1 \\ 0 \\ \vdots \\ 0
		\end{bmatrix}
		%
		\qquad
		%
		\bm{x}^2 = \begin{bmatrix}
		0 \\ 1 \\ \vdots \\ 0
		\end{bmatrix}
		%
		\qquad
		%
		\bm{x}^N = \begin{bmatrix}
		0 \\ 0 \\ \vdots \\ 1
		\end{bmatrix}
	\end{align}
\end{homework}

\section{Matrices}
%
We can write an array of complex numbers in the form 
%
\begin{align}
	X = \begin{bmatrix}
	x_{11} & x_{12} & \ldots & x_{1N} \\
	%
	x_{21} & x_{22} & \ldots & x_{2N} \\
	%
	\vdots & \vdots & \ddots & \vdots \\
	%
	x_{N1} & x_{N2} & \ldots & x_{NN} \\
	\end{bmatrix}
	\label{eq:square_matrix}
\end{align}
%
The matrix of \eqref{eq:square_matrix} shall be called a \textit{square matrix}. The quantities $x_{ij}$ are the \textit{elements} of the matrix $X$; the quantities $x_{i1}, x_{i2}, \ldots,  x_{iN}$ are the $i$th \textit{rows} of the matrix $X$ and the quantities $x_{1j}, x_{2j}, \ldots,  x_{Nj}$ are the $j$th columns of $X$. We denote matrices with upper case letters or the lower-case subscript notations 
%
\begin{align}
	X = \left(x_{ij}\right)
\end{align}
%
while the \textit{determinant} of the array associated with \eqref{eq:square_matrix} shall be denoted $|X|$ or $|x_{ij}$.

Similar to the equality definition between vectors, two matrices are said to be equal if and only if their elements are equal \ie 
%
\begin{align}
	A + B = \left(a_{ij} + b_{ij}\right)
\end{align}

Scalar multiplication of a matrix can be expressed as 
%
\begin{align}
	c_1 X = X c_1 = \left(c_1 x_{ij}\right)
\end{align}

Lastly, by $\bar{X}$ we shall mean the matrix whose elements are the complex conjugates of $X$. $X$ is a real matrix if the elements of $X$ are real.

\subsection{Vector by Matrix Multiplication}

Recall the linear transformation
%
\begin{align}
	y_i = \sum_{j=1}^{N} a_{ij} x_j \qquad i=1,2,\ldots, N
\end{align}
%
where $a_{ij}$ are complex quantities. For two vectors $\bm{x}$ and $\bm{y}$ related as above, we have
%
\begin{align}
	\bm{y} = A \bm{x}
	\label{eq:linear_y}
\end{align}
%
to describe the multiplication of a vector $\bm{x}$ by a matrix $X$. 

\begin{homework}
	Consider the identity matrix $I$, so defined
	\begin{align}
		I = \begin{bmatrix}
		1 & 0 & \ldots & 0 \\
		0 & 1 & \ldots & 0 \\
		\vdots & \vdots & \ddots & \vdots \\
		0 & 0 & \ldots & 1 \\
		\end{bmatrix}
	\end{align}
	\ie $I = (\delta_{ij})$, where $\delta_{ij}$ is the Kronecker delta symbol, defined as
	\begin{align}
		\delta_{ij} = \begin{cases}
		0, \qquad \text{if  } i \neq j \\
		1, \qquad \text{if  }  i=j
		\end{cases}
	\end{align}
	Show that 
	\begin{align}
		\delta_{ij} = \sum_{k=1}^{N} \delta_{ik} \delta_{kj}
	\end{align}
\end{homework}

\begin{homework}
	Show that 
	\begin{align}
		\langle A \bm{x}, A \bm{x} \rangle = \sum_{i=1}^{N} \left(\sum_{j=1}^{N} a_{ij}x_j\right)^2
	\end{align}
\end{homework}

\subsection{Matrix by Matrix Multiplication}

Consider \eqref{eq:linear_y}. Now, suppose our goal is to generate a second-order linear transformation so defined 
%
\begin{align}
	\bm{z} = B\bm{y}
\end{align}
%
which converts the components of $\bm{y}$ into components of $\bm{z}$. To express the components of $\bm{z}$ in terms of the components of $\bm{x}$ this, we write
%
\begin{align}
	z_i &= \sum_{k=1}^{N} b_{ik} y_k = \sum_{k=1}^{N} b_{ik} \left(\sum_{j=1}^{N}a_{kj}x_j\right) \\
	&= \sum_{j=1}^{N} \left(\sum_{k=1}^{N}b_{ik}a_{kj}\right)\bm{x}_j
\end{align}

Introducing $C=\left(c_{ij}\right)$ defined as 
%
\begin{align}
	c_{ij} = \sum_{k=1}^{N} b_{ik} a_{kj} \qquad i,j = 1,2,\ldots,N
\end{align}
%
we may write 
%
\begin{align}
	\bm{z} = C \bm{x}
\end{align}
%
Since, formally
%
\begin{align}
	\bm{z} = B\bm{y} = B(A\bm{x}) = B(A\bm{x}) = (BA) \bm{x}
\end{align}
%
so that 
%
\begin{align}
	C = BA
\end{align}
%
Note the ordering of the matrix product above.
%
\begin{homework}
	Show that 
	%
	\begin{align}
		f(\theta_1) f(\theta_2) = f(\theta_2) f(\theta_1) = f(\theta_1 + \theta_2)
	\end{align}
	%
	where 
	\begin{align}
		f(\theta) = \begin{bmatrix}
		\cos \theta & -\sin \theta \\
		\sin  \theta & \cos \theta
		\end{bmatrix}
	\end{align}
\end{homework}

\begin{homework}
	Show that 
	\begin{align}
		\left(a_1^2 + a_2^2\right)\left(b_1^2 + b_2^2\right) = \left(a_2 b_1 + a_1 b_2\right)^2 + \left(a_1 b_1 - a_2 b_2\right)^2
	\end{align}
	\textbf{Hint:}  $|A B | = |A| |B|$, 
\end{homework}

\subsection{Non-Commutativity}

Matrix multiplication is not commutative, \ie $AB \neq BA$. For an example, consider the following $3\times3$ matrices
%
\begin{align}
	A = \begin{bmatrix}
	5 & 6 & 9 \\
	2 & 1  & 6 \\
	3 & 6 & 9
	\end{bmatrix} 
	\qquad 
	B = \begin{bmatrix}
	1   &  4  &  13 \\
	23  &   6  &  24 \\
	8   &  3    & 9
	\end{bmatrix} 
\end{align}
%
where
%
\begin{align}
	AB = \begin{bmatrix}
	215  &  83 &  290 \\
	73  &  32 &  104 \\
	213  &  75 &  264
	\end{bmatrix} 
	%
	\qquad
	%
	\text{ and }
	%
	BA = \begin{bmatrix}
	52   & 88  & 150 \\
	199 &  288 &  459 \\
	73 &  105 &  171
	\end{bmatrix} 
\end{align}
%
so that $AB \neq BA$. If, however, $AB = BA$, we say $A$ and $B$ \textit{commute}. Note that 
%
\begin{align}
	(AB)^{-1} = B^{-1} A^{-1}.
\end{align}

\subsection{Associativity}
Associativity of matrix multiplication gets preserved unlike the commutativity. So for matrices $A$, $B$, and $C$, we have
%
\begin{align}
	\left(AB\right)C = A\left(BC\right)
\end{align}
%
that is, the product $ABC$ is unambiguously defined without the parentheses. To prove this, we write the $ij$th element of $AB$ as 
%
\begin{align}
	a_{ik} b_{kj}
\end{align} 
%
so that the definition of multiplication implies that 
%
\begin{align}
	\left(AB\right) C &= \left[\left(a_{ik} b_{kl} \right)c_{lj}\right] \\
	A \left(BC\right) & = \left[a_{ik}\left( b_{kl} c_{lj} \right)\right]
\end{align}
%
which establishes the equality $\left(AB\right) C$ and $A \left(BC\right)$.

\subsection{Invariant Vectors}
%
The problem of finding the minimum or maximum of  $Q = \sum_{i,j=1}^{N} a_{ij} \bm{x}_i \bm{x}_j$ for $\bm{x}_i$ satisfying the relation $\sum_{i=1}^{N} \bm{x}_i^2 = 1$ can be reduced to the problem of finding the values of the scalar $\lambda$ that satisfies the set of linear homogeneous equations
%
\begin{align}
	\sum_{j=1}^{N} a_{ij} \bm{x}_j = \lambda \bm{x}_i, \qquad i=1,2,\ldots, N
\end{align}
%
which possesses nontrivial solutions. Vectorizing, we have
%
\begin{align}
	A \bm{x} = \lambda \bm{x}
\end{align}
%
Here, $\bm{x}$ signifies the direction indicated by the $N$ direction numbers $\bm{x}_1, \bm{x}_2, \ldots, \bm{x}_N$, and we are searching for the directions  that are invariant.

\subsection{The Matrix Transpose}

We define the transpose of the matrix $A = (a_{ij})$ as $A^T = (a_{ji})$ \ie the rows of $A^T$ are the columns of $A$ and vice versa. An important consequence of this is that the transformation $A$ on the set of a vector $\bm{x}$ is same as the transformation of the matrix $A^T$ on the set $\bm{y}$. This is shown in the following
%
\begin{align}
	\langle A\bm{x}, \bm{y} \rangle  = y_1 \sum_{j=1}^{N} a_{1j} \bm{x}_j +  y_2 \sum_{j=1}^{N} a_{2j} \bm{x}_j +  \ldots +  y_N \sum_{j=1}^{N} a_{Nj} \bm{x}_j 
\end{align}
%
which becomes upon rearrangement,
%
\begin{align}
		\langle A\bm{x}, \bm{y} \rangle  & = x_1 \sum_{i=1}^{N} a_{i1} \bm{y}_i +  x_2 \sum_{i=1}^{N} a_{i2} \bm{y}_i +  \ldots +  x_N \sum_{i=1}^{N} a_{iN} \bm{y}_i  \\
		&= \langle \bm{x}, A^T \bm{y} \rangle 
\end{align}

We can then regard $A^T$ as the \textit{induced or adjoint transformation} of $A$. An interesting property of the transpose of amatrix product is that 
%
\begin{align}
	(AB)^T = B^T A^T
\end{align}

\subsection{Symmetric Matrices}
Matrices that satisfy the relation 
\begin{align}
	A = A^T
\end{align}
%
play a crucial role in the study of quadratic forms and such matrices are said to be \textit{symmetric}, with the property that 
%
\begin{align}
	a_{ij} = a_{ji}
\end{align}

\begin{homework}
	Prove that $(A^T)^T = A$
\end{homework}
%
\begin{homework}
	Prove that $\langle A\bm{x}, B\bm{y}\rangle = \langle \bm{x}, A^T B \bm{y} \rangle$
\end{homework}

\subsection{Hermitian Matrices}

The scalar function for complex vectors is the expression $\langle \bm{x}, \bm{\bar{y}}\rangle$. Suppose we define $\bm{z}=\bar{A^T} \bm{y}$, then 
%
\begin{align}
\langle A \bm{x}, \bar{y}\rangle =\langle \bm{x}, \bar{\bm{z}}\rangle 
\end{align}
%
\ie the induced transformation is now $\bar{A^T}$, the complex conjugate of $A$. Matrices for which 
%
\begin{align}
	A = \bar{A^T}
\end{align}
%
are called Hermitian. Note that in some literature, the Hermitian matrix is often written as $A^\star$.

\subsection{Orthogonal Matrices}
%
This section has to do with the invariance of distance between matrices, that is, taking the Euclidean measure of distance as the measure of the magnitude of the real-valued vector $\bm{x}$. The prodding question of interest is to figure out the linear transformation $\bm{y} = H \bm{x}$ that leaves the inner product $\langle\bm{x}, \bm{z}\rangle$. Mathematically, we express this problem such that 
%
\begin{align}
	\langle \bm{x}, \bm{x}\rangle = \langle H\bm{x}, H \bm{x} \rangle 
	\label{eq:orthogonal}
\end{align}
%
is satisfied for \textit{all} $\bm{x}$. We know that  
%
\begin{align}
	\langle H \bm{x}, H \bm{x}\rangle = \langle \bm{x}, H^T H \bm{x}\rangle
\end{align}
%
and that $H^TH$ is symmetric so that  \eqref{eq:orthogonal}, gives 
%
\begin{align}
	H^TH = I.
	\label{eq:real_invar}
\end{align}
%
\begin{tcolorbox}[title=Orthogonal Matrix]
	A real matrix $H$ for which $H^TH=I$ is called \textit{orthogonal}.
\end{tcolorbox}

\subsection{Unitary Matrices}

This is the measure of the distance of a complex vector, akin to the invariance condition of real-valued matrices \eqref{eq:real_invar}. We define the unitary property as follows:
%
\begin{align}
	H^\star H = I.
\end{align}
%
Matrices defined as in the foregoing play a crucial role in the treatment of Hermitian matrices, such as the role that orthogonal matrices play in symmetric matrices theory.

\subsection{Matrix Determinant}
%
The determinant of a scalar is same as the scalar while the determinant of a matrix shall be inductively defined for square matrices. Suppose we have an $n \times n$ matrix $A$, its determinant is defined as 
%
\begin{align}
	|A| = \sum_{j=1}^N (-1)^{i+j} a_{(i,j)} | a^{(i,j)} |
	\label{eq:laplace_exp}
\end{align}
%
for any value of $i \in [1, n]$, where \eqref{eq:laplace_exp} is called the Laplace expansion of $|A|$ along its $i$th row. Equation \eqref{eq:laplace_exp} shows us that the determinant of the square matrix $A$ is found in terms of the determinants of the $(n-1) \times (n-1)$ matrices. Similarly, the determinants of $(n-1) \times (n-1)$ matrices are defined by $(n-2) \times (n-2)$ and so on until we get to the determinant of $1\times1$ matrices which are scalars. We can also define the determinant of $A$ as 
%
\begin{align}
	 |A| = \sum_{i=1}^N(-1)^{i+j} a_{(i,j)} |a^{(i,j)}|
\end{align}
%
for any value of $j \in [1, n]$. This is termed the Laplace expansion of $A$ along its $j$th column. It follows that 
%
\begin{subequations}
	\begin{align}
	   |A_{11}| &= A_{11} 
	\end{align}
	%
	\begin{align}
	\text{det} \begin{bmatrix}
	A_{11} & A_{12} \\
	%
	A_{21} & A_{22}
	\end{bmatrix} = A_{11} A_{22} - A_{12}A_{21}
	\end{align}
	%
	\text{and that}
	%
	\begin{align}
	\text{det} \begin{bmatrix}
	A_{11} & A_{12} & A_{13} \\
	%
	A_{21} & A_{22} & A_{23} \\
	%
	A_{31} & A_{32} & A_{33} 
	\end{bmatrix} &= A_{11} (A_{22} A_{33}- A_{23}A_{32}) - \\
						  & A_{12} (A_{21} A_{33}- A_{23}A_{31}) + \\
						  & A_{13} (A_{21} A_{32}- A_{22}A_{31}) \
	\end{align}
\end{subequations}

\subsection{Properties of the Matrix Determinant}
\begin{enumerate}
	\item $|AB| = |A| |B|$, where $A$ and $B$ are assumed to be of equal dimensions.
	%
	\item $|A| = \prod_{i=1}^{N} \lambda_i$, where $\lambda_i$ are the eigenvalues of $A$.
	%
	\item The inverse of $A$ is said to exist if $AA^{-1} = I$. Such a matrix is said to be \textit{non-singular}. Note that $A$ must be a square matrix in order for it to have a determinant. A square matrix whose inverse does not exist is said to be \textit{singular}. 
\end{enumerate}
%
Take for example,
%
\begin{align}
	\begin{bmatrix}
	3 & 0 \\ 2, 1
	\end{bmatrix}
	%
	 \begin{bmatrix}
	1/3 & 0 \\ -2/3, 1
	\end{bmatrix} = 
	%
	 \begin{bmatrix}
	1 & 0 \\ 0, 1
	\end{bmatrix}.
\end{align}
%
Then we say that the two matrices on the left are inverses of one another. Among other ways of stating the nonsingularity of $A$ are that 
%
\begin{itemize}
	\item $A$'s rows or columns are linearly independent.
	%
	\item $|A| \neq 0$.
	%
	\item $Ax = b$ has a unique solution $x$ for all $b$.
	%
	\item The rank of $A=n$.
	%
	\item $0$ is not an eigenvalue of $A$. 
	%
	\item $A^{-1}$ exists.
\end{itemize}

\subsection{The Matrix Trace}
%
The trace of a matrix exists if and only of the matrix is square. It is defined as the sum of its diagonal elements.
%
\begin{align}
	\text{Tr}(A) = \sum_{i=1} A_{ii}
\end{align}
%
Also, the trace can be expressed in terms of the sum of the matrix's eigenvalues,
%
\begin{align}
\text{Tr}(A) = \sum_{i=1} \lambda_i.
\end{align}
%
The trace of a matrix product is not dependent in the order of multiplication of the matrices:
%
\begin{align}
	\text{Tr}(AB) = \text{Tr}(BA).
\end{align}

\subsection{Eigenvectors and Eigenvalues of a Matrix}

A square $n\times n$ matrix $A$ has $n$ eigenvalues and $n$ eigenvectors. If 
%
\begin{align}
	A \bm{x} = \lambda \bm{x}
\end{align}
%
for a scalar $\lambda$ and an $n \times 1$  vector $\bm{x}$ then we say the matrix $A$ has eigenvalues $\lambda$ and eigenvectors $\bm{x}$. Together, $\lambda$ and $\bm{x}$ are called \textit{eigendata}, the \textit{characteristic roots}, \textit{latent roots}, or \textit{proper numbers and vectors} of the matrix.

\begin{homework}
	If $A$ has eigendata $\left(\lambda, \bm{x}\right)$, show that $A^2$ has eigendata $\left(\lambda^2, \bm{x}\right)$. 
\end{homework}
%
\begin{homework}
	Show that $A^{-1}$ exists if and only if none of the eigenvalues of $A$ are zero.
\end{homework}
%
\begin{homework}
	Show that the eigenvalues of $A$ are real numbers if $A$ is symmetric.
\end{homework}

\subsection{Other Matrix Properties}
%
A \textit{symmetric} $n \times n$ matrix $A$ can be characterized as either positive definite, positive semidefinite, negative definite, negative semidefinite, or indefinite if matrix $A$ is
%
\begin{itemize}
	\item \textit{Positive definite} if $\bm{x}^TA\bm{x}>0$ for all nonzero $n \times 1$ vectors $\bm{x}$. That is, all the eigenvalues of $A$ are positive real numbers.  If $A$ is positive definite, then so is $A^{-1}$.
	%
	\item \textit{Positive semidefinite} if $\bm{x}^TA\bm{x} \ge 0$ for all nonzero $n \times 1$ vectors $\bm{x}$. That is, all the eigenvalues of $A$ are non-negative real numbers. A positive semidefinite matrices are sometimes called nonnegative definite.
	%
	\item \textit{Negative definite} if $\bm{x}^TA\bm{x} < 0$ for all nonzero $n \times 1$ vectors $\bm{x}$. That is, all the eigenvalues of $A$ are negative real numbers.  If $A$ is negative definite, then so is $A^{-1}$.
	%
	\item \textit{Negative semidefinite} if $\bm{x}^TA\bm{x} \le 0$ for all nonzero $n \times 1$ vectors $\bm{x}$. That is, all the eigenvalues of $A$ are non-negative real numbers. A positive semidefinite matrices are sometimes called non positive definite.
	%
	\item When some of the eigenvalues of $A$ are positive and some are negative, then the matrix is said to be \textit{indefinite}.
\end{itemize}

The singular values of matrix $A$ are defined as 
%
\begin{align}
	\sigma^2 (A) &= \lambda (A^TA) \nonumber \\
						&= \lambda (A^TA)
\end{align}
%
For an $n \times n$ matrix $A$, we have a $\min(n,m)$ singular values. If $n > m$, then $AA^T$ will have the same eigenvalues as $A^TA$ and an additional $n-m$ zeroes. We do not consider the additional zeroes to be singular values of $A$ because $A$ always has $\min (n, m)$ singular values.
%
\begin{quiz}
	If $A$ is $n \times m$, what are number of eigenvalues of $A^TA$ and $AA^T$ respectively?
\end{quiz}




\subsection{The Matrix Inversion Lemma}
%
This is sometimes called the \textit{Woodbury matrix identity}, named after Max A. Woodbury., \textit{Sherman-Morrison formula}, or the \textit{modified matrices formula}.  It a tool frequently used in statistics, system identification, state estimation and control theory. Assume we have a blockwise matrix 
%
$\left(\begin{array}{cc}
	A & B \\ C & D
\end{array}\right)$
%
where $A$ and $D$ are invertible square matrices, and $B$ and $C$ are not necessarily square. We can define the following matrices
%
\begin{align}
	E &= D - C A^{-1} B \nonumber \\
	F &= A - BD^{-1} C.
\end{align}
%
If $E$ is invertible, it follows that
%
\begin{align}
	\begin{bmatrix}
		A & B \\ C & D
	\end{bmatrix}
	%
		\begin{bmatrix}
	A^{-1}  + A^{-1} B E^{-1} C A^{-1} & -A^{-1}B E^{-1} \\ -E^{-1} C A^{-1} & E^{-1}
	\end{bmatrix}
	=
	\begin{bmatrix}
	I & 0 \\ 0 & I
	\end{bmatrix}.
	\label{eq:inv_lemma1}
\end{align}
%
Also, if $F$ were invertible, it follows that 
%
\begin{align}
\begin{bmatrix}
A & B \\ C & D
\end{bmatrix}
%
\begin{bmatrix}
F^{-1} & -A^{-1} B E^{-1} \\ -D^{-1} C F^{-1} & E^{-1}
\end{bmatrix}
=
\begin{bmatrix}
I & 0 \\ 0 & I
\end{bmatrix}.
\label{eq:inv_lemma2}
\end{align}
%
It follows that \eqref{eq:inv_lemma1} and \eqref{eq:inv_lemma2} are two expressions for the inverse of $\left(\begin{array}{cc}
A & B \\ C & D
\end{array}\right)$. We must therefore have the upper-left partitions of the two matrices equal so that 
%
\begin{align}
	F^{-1} = A^{-1} + A^{-1} B E^{-1} C A^{-1}
\end{align}
%
and from the definition of $F$, we have
%
\begin{align}
 \left(A - B D^{-1}C\right)^{-1} = A^{-1} + A^{-1} B \left(D - C A^{-1} B\right)^{-1} C A^{-1}
\end{align}
%
An alternative statement of the matrix inversion lemma is
%
\begin{align}
\left(A -+B D^{-1}C\right)^{-1} = A^{-1} + A^{-1} B \left(D + C A^{-1} B\right)^{-1} C A^{-1}
\end{align}
%
\begin{quiz}
	Verify the expressions in \eqref{eq:inv_lemma1} and \eqref{eq:inv_lemma2}.
\end{quiz}

\begin{example}
	\label{ex:mat_inv_lemma}
	Suppose that at Brandeis, you took three courses in your Freshman year namely, RBOT 101, RBOT 103, and RBOT 105 where you got $90\%$, $85\%$, and $86\%$ respectively. In your Sophomore year, you took  RBOT 201, RBOT 203, and RBOT 205, where you got $65\%$, $68\%$, and $92\%$ respectively, and in your junior year, you decide to retake your Sophomore classes, where your scores increased by $10\%$, $5\%$, on the first two courses and decreased by $8\%$ in the last course. Your GPA each year increased by $4\%$, $3.5\%$ and $2.5\%$  respectively. Given your analytical prowess, you decide to model each year's GPA changes with the equation $z = a u + b v + c w$, where $u$ and $v$ and $w$ are the scores/grades you got as percentages and $a$, $b$, and $c$ are unknown constants. To find the unknown constants, you figure you need to invert the matrix
	%
	\begin{align}
		A = \begin{bmatrix}
			90 & 85 & 86 \\
			65 & 68 & 92 \\
			71.5 & 71.4 & 84.64
		\end{bmatrix}
	\end{align}
	%
	so that 
	%
	\begin{align}
	A^{-1} = \begin{bmatrix}
	23/20 & 155/104 & -145/52 \\
	-207/136 & -569/274 & 6725/1768 \\
	5/16 & 205/416 & -175/208
	\end{bmatrix}
	\end{align}
	%
	It follows that the unknown constants are 
	%
	\begin{align}
		X = A^{-1} \left(\begin{array}{c}
			4 \\ 7/2 \\ 5/2
		\end{array}\right) = \left(\begin{array}{c}
		2959/1040 \\ -2693/700 \\ 725/832
		\end{array}\right) 
	\end{align}
	%
	As a result, you are able to determine a model which allows you to predict future GPA changes based on how hard you work, sleep, engage in social activities. You can better allocate your time resource and improve your grades in the following years.
	
	Suppose that in the aftermath of generating this model, you now realize that your grade in RBOT 201  the second year was $86\%$ rather than $65\%$, this means that in order to find the constants, you want to invert
	%
	\begin{align}
	\bar{A} = \begin{bmatrix}
	90 & 85 & 86 \\
	86 & 68 & 92 \\
	71.5 & 71.4 & 84.64
	\end{bmatrix}.
	\end{align}
	%
	Rather than invert all the matrix all over, you decide to apply a mathematical trick leveraging the inversion lemma. You write $\bar{A} = A + BD^{-1}C$, where 
	%
	\begin{subequations}
		\begin{align}
			B = \begin{bmatrix}
					0 & 21 & 3.44
			\end{bmatrix}^T
			%
			C = \begin{bmatrix}
			1 & 0 & 0
			\end{bmatrix}
			%
			D = 1
		\end{align}
	\end{subequations}
so that 
%
\begin{align}
	\bar{A}^{-1} &= \left(A + BD^{-1}C\right)^{-1} \\
						&= A^{-1} - A^{-1} B\left(D + C A^{-1}B\right)^{-1} C A^{-1}
\end{align}
%
The $\left(D + C A^{-1}B\right)^{-1}$ term turns out to be a scalar so that 
%
\begin{align}
\bar{A}^{-1} &= \begin{bmatrix}
0.0506 & 0.0656 & -0.1228 \\
%
0.0239 & -0.073 & 0.0551 \\
%
-0.065 & 0.0035 & 0.0741
\end{bmatrix}
\end{align}
%
\begin{align}
	X &= \bar{A}^{-1} \left(\begin{array}{c}
	4 \\ 7/2 \\ 5/2
	\end{array}\right) \nonumber \\
	%
	&= 
	\left(\begin{array}{c}
	51/407   \\
	-134/6037  \\
	-148/2361 
	\end{array}\right)
\end{align}
%
Here, the matrix inversion lemma may not be necessary since the size of the matrix is small. However, if the matrix had a larger size, the computational savings of using the matrix inversion lemma becomes appreciated.

\end{example}

\begin{homework}
	Using the same linear model employed in \ref{ex:mat_inv_lemma}, suppose that on a typical weekend, you go to your local Farmers' market and bought tomatoes, bell peppers, and blue berries for $\$35\%$, $\$18\%$, and $\$32\%$ respectively; on your way home, you drove to your local grocery store and found that the prices for each item were actually increased by $10\%$, $25\%$ for each of tomatoes, and bell peppers, and decreased by $68\%$  for the blue berries. You decide to buy more blueberries that cost a total of $\$50$ at your local grocery; and discovered that $\$5$ worth of tomatoes bought at the Farmers' market was defective and had to be discarded. Can you compute a model that allows you to predict future prices for good tomatoes, blue berries and bell peppers at your local Farmers' market?
\end{homework}