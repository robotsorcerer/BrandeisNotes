\chapter{Objects' Alignment in Robotics.}  
 \label{chap:opt_rot}
 
 In this chapter, we are concerned with the problem of optimally aligning two vectors, a model point/shape to a ``sensed" point/shape in space \eg $\nu_1, \nu_2 \in \mathbb{R}^n$ to one another with the minimal amount of errors. There exists many methods of solving this problem. Most of them leverage clever optimization methods and we will be looking into these in this chapter. We could follow the homogeneous transformation scheme we presented in Chapter \ref{chap:intro}, but we would not have an optimal solution. A popular technique in computer geometry and computer vision is to use the iterative closest point algorithm(ICP), an algorithm by Paul Besl and Neil McKay developed out of General Motors Laboratory in the 1990's~\cite{besl1992method}. This is more appropriate for 3D tasks and it describes a generic, representation method for the accurate and computationally efficient registration of three-dimensional (3-D) shapes. The ICP algorithm always converges monotonically to the nearest local minimum of a mean-square distance metric such as an $l-2$ distance, and this convergence rate is of the order of a few iterations. An important property of the ICP algorithm is that it can register data from unfixtured rigid objects with an ideal geometrical model prior to shape inspection. So, if we want to figure out that two geometric representations are congruent, estimate the motion between them in real-time where the correspondences are not known, ICP tends to be really good for such operations.
 
 Now, suppose our dataset is not a complex geometric primitive\footnote{We shall refer to a geometric primitive as a primitive 3D shape such as a cylinder, square, prism and the likes.}, but rather a set of two vectors such that we are tasked with the problem of determining the best \textit{unconstrained transformation} between the two sets of coordinates. We can formulate the problem into a constrained optimization problem and thereafter, through clever factorization, turn the problem into a simple one of factorizing the unconstrained transformation  into a symmetric and orthogonal matrix by which we may solve for the optimal rotation and translation. The algorithm we shall be looking into will be the one that was invented in crystallography in 1976 and updated in 1978 by Wolfgang Kabsch, today dubbed the Kabsch algorithm~\cite{Kabsch1978}. Kabsch showed that a direct solution was possible, irrespective of the non-linear character of the problem.
 
 While other newer algorithms exist, these are the two popular algorithms that we shall be concerning ourselves with in this chapter.
 
 
 \section{Preliminaries}
 \seclabel{sec:opt_rot_prelim}
 
 We will denote the real line by $\mathbb{R}$. An example of a \textbf{metric space} is the \textbf{Euclidean } $n$-\textbf{space} $\mathbb{R}^n$, which consists of $n-$tuples $x = \left(x_1, x_2, \ldots, x_n\right)$ where each $x_i \in \mathbb{R}$. We shall mean an $\mathbb{R}^n$ metric space to have the metric 
 %
 \begin{align}
 d(x,y) = \sqrt{\sum_{i=1}^{n} (y_i - x_i)^2}.
 \end{align} 
 %
 If $n=0$, then $\mathbb{R}^0$ is taken to be a single point $0 \in \mathbb{R}$.
 
 A manifold is ``locally" similar to one of the example metric spaces $\mathbb{R}^n$. Precisely, a \textbf{manifold} is a metric space $\bm{M}$ with the property that, \textit{if $x \in M$., then there is some neighborhood $U$ of x and some integer $n \ge 0$ such that $U$ is homeomorphic to $\mathbb{R}^n$}. 
 
 A simple example of a manifold is $\mathbb{R}^n$: for each $x \in \mathbb{R}^n$ we can take $U$ to be everything in $\mathbb{R}^n$. 
 
 \begin{quiz}
 	 Suppose we supply $\mathbb{R}^n$ with an equivalent metric, which makes it homeomorphic to $\mathbb{R}^n$, would it also be a manifold?
 \end{quiz}
 
 Another example of a metric space is an open ball in $\mathbb{R}^n$, wherein one can take $U$ to be the entire open ball since an open ball in $\mathbb{R}^n$ is homeomorphic to $\mathbb{R}^n$. Similarly, an open subset $V$ of $\mathbb{R}^n$ is a manifold, \ie for each $x \in V$ we can choose $U$ to be some open ball with $x\in U \subset V$.
 
 
 The \textbf{Euclidean distance} $d(\bm{r}_1, \bm{r}_2)$ between two points $\bm{r}_1 = (x_1, y_1, z_1)$ and $\bm{r}_2 = (x_2, y_2, z_2)$ is given by 
 %
 \begin{align}
 d(\bm{r}_1, \bm{r}_2) = \|\bm{r}_1 - \bm{r}_2\| =\sqrt{(x_2-x_1)^2 + (y_2-y_1)^2 + (z_2-z_1)^2}.
 \end{align}
 
 Suppose that $P$ is a point set with $N_p$ points denoted as $\bm{p}_i: P=\{p_i\}$ for $i=1,\ldots,N_p$. The distance between the point $\bm{q}$ and the point set $P$ is
 %
 \begin{align}
 	d(\bm{q}, P) = \min_{i\in \{1,\ldots,N_p\}}{d(\bm{q}, \bm{p}_i)}.
 \end{align}
 %
 We find that the closest point $\bm{p}_j$ of $P$  satisfies $d(\bm{q}, \bm{p}_j) = d(\bm{q}, P)$.
 
 Suppose that we have a \textbf{line segment} that connects the points, $bm{r}_1, \bm{r}_2$, the distance between the point $\bm{r}$ and the line segment $l$ is
 %
 \begin{align}
 	d(\bm{p}, l) = \min_{x+y=1} \|x\bm{r}_1 + y \bm{r}_2 - \bm{p} \|
 	\label{eq:line_seg}
 \end{align}
 %
 where $x, y \in [0, 1 ]$.
 %
 \begin{homework}
 	Find a closed-form expression for the solution to \eqref{eq:line_seg}.
 \end{homework}
%
Now, if instead of a line segment, suppose we have a set of $N_l$ line segments denoted $l_i$, and let $L=\{l_i\}$ for $i=1,\ldots,N_l$. The \textbf{distance between the point $\bm{p}$ and the line segment set $L$} is 
%
\begin{align}
	d(\bm{p}, L) = \min_{i\in \{1,\ldots, N_l\}} d(\bm{p}, l_i).
\end{align}
%
The closest point $y_j$ on the line segment set $L$ satisfies $d(\bm{p}, y_j) = d(\bm{p}, L)$.
%
Let $g$ be a triangle with the following coordinates  $\bm{r}=(x_1,y_1,z_1), \bm{r}_2 = (x, y, z)$, and $\bm{r}_3 = (x_3,y_3,z_3)$. The \textbf{distance between the point $\bm{p}$ and the triangle $g$} is 
%
\begin{align}
	d(\bm{p}, g) = \min_{x+y+z=1} \| x\bm{r}_1 + y\bm{r}_2 + z \bm{r}_3 - \bm{p} \|
	\label{eq:triangle_dist}
\end{align}
%
where $x \in [0,1], \,\, y\in [0,1],$ and $z \in [0,1]$. 
%
\begin{homework}
	Find a closed-form expression for the problem in \eqref{eq:triangle_dist}.
\end{homework}
 
 Now, if we have a collection of $N_g$ triangles $G$, denoted by $g_i$ such that $G =\{g_i\}$ for $i=1,\ldots,N_g$. The \textbf{distance between the point $\bm{p}$ and the triangle set $G$} is 
 %
 \begin{align}
 	d(\bm{p}, G) = \min_{i\in \{1,\ldots, N_g\}} d(\bm{p}, g_i),
 \end{align}
 %
 and the closest point $y_j$ on the triangle set $G$ satisfies the equality $d(\bm{p}, y_j) = d(\bm{p}, G)$.
 
 \subsection{Distance between a Point and a  Parameterized  Entity}
 %
We define a parametric curve and a parametric surface as single parametric entities $\bm{r}(\bm{u})$, where $\bm{u} = u\in \mathbb{R}^1$ denotes a parameterized curve, and $\bm{u} = (u,v) \in \mathbb{R}^2$ denotes parametric surfaces. We will evaluate a curve within an interval domain \eg $[x,y]$ while the evaluation domain of a surface can be an arbitrarily closely-connected region in a plane. 

We will take the distance from a given point $\bm{p}$ to a parametric entity $E$ to be 
%
\begin{align}
	d(\bm{p}, E)  = \min_{\bm{r}(\bm{u}) \in E} d(\bm{p}, \bm{r}(\bm{u}))
\end{align}
 %
 To compute the point-to-curve and point-to-surface distances, let $F$ be the set of $N_e$ parametric entities denoted by $E_i$, and let $F = \{E_i\}$ for $i = 1, N_e$> The distance between a point $\bm{p}$ and the parametric entity set $F$ is
 %
 \begin{align}
 	d(\bm{p}, F) = \min_{i\in \{1,\ldots, N_e\}} d(\bm{p}, \bm{E}_i).
 \end{align}
 
 To find the distance from a point to a parametric entity, we can create a simplex-based approximation for \eg a line segment or triangle. For a parametric space curve $C =\{\bm{r}(u)\}$, we can compute a polyline $L(C, \delta)$ such that the piecewise-linear approximation never deviates from the space curve by more than a prespecified distance $\delta$. If we tag every point of the polyline with a corresponding $u$ argument values of the parametric curve, we can obtain an estimate of the closest point from the line segment set.
 
 In a similar vein, for a parametric surface $S = \{\bm{r}(u, v)\}$, one can compute a triangle set $G(S, \delta)$ such that the piecewise triangular approximation never deviates from the surface by more than a prespecified distance $\delta$. If we tag each truangle vertex with the corresponding $(u, v)$ argument values of the parametric surface, we can find the $U_a, v_a)$ of the argument values of the closest point from the triangle set. The initial value of $\bm{u}_a$ is assumed to be available such that $\bm{r}(\bm{u}_a)$ is very close to the closest point on the parametric entity.
 
 We can employ a Newtonian minimization approach for solving the point to parametric entity problem when a reliable starting point $\bm{u}_a$ is available. The scalar objective function to be minimized is 
 %
 \begin{align}
 	f(\bm{u}) = \| \bm{r}(\bm{u}) - \bm{p} \|^2.
 \end{align}
 %
 Suppose $\Delta = \left[\partial/\partial \bm{u}\right]^T$ is the vector differential gradient operator, the minimum of $f$ must occur at $\Delta f = 0$. If we have a surface, then we must have $\Delta f = \left[f_u, f_v\right]^T$, with the 2-D Hessian matrix is given by
 %
 \begin{align}
 	\Delta \Delta^T(f) = \begin{bmatrix}
 	f_{uu} & f_{uv} \\
 	f_{uv} & f_{vv}
 	\end{bmatrix}
 \end{align}
 %
 where the partial derivatives of the objective function is
 %
 \begin{subequations}
 	\begin{align}
 	f_u(\bm{u}) &= 2 \bm{r}_u^T(\bm{u})(\bm{r}(\bm{u})-\bm{p}) \\
 	f_v(\bm{u}) &= 2 \bm{r}_v^T(\bm{u})(\bm{r}(\bm{u})-\bm{p}) \\
 	f_{uu}(\bm{u}) &= 2 \bm{r}_{uu}^T(\bm{u})(\bm{r}(\bm{u})-\bm{p}) + 2 \bm{r}_u^T(\bm{u})  \bm{r}_u(\bm{u}) \\
 	f_{vv}(\bm{u}) &= 2 \bm{r}_{vv}^T(\bm{u})(\bm{r}(\bm{u})-\bm{p}) + 2 \bm{r}_v^T(\bm{u})  \bm{r}_v(\bm{u}) \\
 	f_{uv}(\bm{u}) &= 2 \bm{r}_{uv}^T(\bm{u})(\bm{r}(\bm{u})-\bm{p}) + 2 \bm{r}_u^T(\bm{u})  \bm{r}_v(\bm{u}).
 	\end{align}
 \end{subequations}
%
And the update relation for the curve and surface case is
%
\begin{align}
	\bm{u}_{k+1} = \bm{u}_k - \left[\Delta \Delta^T(f)(\bm{u}_k)\right]^{-1} \Delta f(\bm{u}_k)
\end{align}
 %
 where $\bm{u}_0 = \bm{u}_a$. 
  
 \section{Kabsch Algorithm}
 
 Suppose we have two sets of vectors $\bm{x_n}$ and $\bm{y_n}$ where $n=1,\ldots, N$, and weight $w_n$ that corresponds to each pair $\bm{x_n}$ and $\bm{y_n}$. Our goal is to find an orthogonal matrix $\cup = (u_{ij})$ which minimizes the cost function
 %
 \begin{align}
 	C = \frac{1}{2} \Sigma_n w_n \left(\cup \bm{x_n} - \bm{y_n}\right)^2 
 	\label{eq:kabsch_unconstraint}
 \end{align}
 %
 subject to 
 %
 \begin{align}
 	\sum_k u_{ki} u_{kj} - \delta_{ij} = 0
 	\label{eq:kabsch_constraint}
 \end{align}
 %
 where $\delta_{ij}$ are the elements of a unit matrix. When there is a translation, we can find the centroid of the vector sets to the origin.
 
 In order to solve the problem, we may introduce a symmetric Lagrangian matrix of multipliers, $L = (l_{ij})$ and an auxiliary function as follows
 %
 \begin{align}
 	D = \frac{1}{2} \Sigma_{i,j} l_{ij} \left(\Sigma_k u_{kl} u_{kj} - \delta_{ij}\right)
 \end{align}
 %
 so that we can form the Lagrangian, $E= C+D$. For each condition in \autoref{eq:kabsch_constraint}, we have an independent number $l_{ij}$ so that the constrained minimum of $C$ is part of the free minima of $D$. A free minimum of $D$ can occur if
 %
 \begin{align}
 	\frac{\partial E}{\partial u_{ij}} = \sum_k u_{ik} \left(\Sigma_n w_n x_{nk}x_{nj} + l_{k,j}\right) - \sum_n w_n y_{nl} x_{nj} = 0
 	\label{eq:kabsch_deri1}
 \end{align}
 %
 and 
 %
 \begin{align}
 	\frac{\partial^2 E}{\partial u_{mk} \partial u_{ij}} = \delta_{mi} \left(\Sigma_n w_n x_{nk} x_{nj} + l_{kj}\right)
 	\label{eq:kabsch_deri2}
 \end{align}
 %
 are elements of a positive definite matrix $x_{nk}$ and $y_{nk}$ are the $k$th elements of $\bm{x_n}$ and $\bm{y_n}$. Now, suppose we have a matrix $R=(r_{ij})$ and a symmetric matrix $\bm{S} = \left(s_{ij}\right)$, such that
 %
 \begin{align}
 	r_{ij} = \sum_n w_n y_{ni} x_{nj}
 \end{align}
 %
 and
 %
 \begin{align}
 	s_{ij} = \sum_n w_n x_{ni} x_{nj}.
 \end{align}
 %
 If the matrix \eqref{eq:kabsch_deri2} has $1$ along its diagonal, we must have the minimum of the Lagrangian E to mean that $S+L$ is positive definite, and \eqref{eq:kabsch_deri1} translates to 
 %
 \begin{align}
 U . \left(S+L\right) = R.
 \label{eq:pre_rot}
 \end{align}
 %
 Our goal would be to find a matrix $L$ of Lagrange multipliers so that $\cup$ is orthogonal. We can do this by multiplying both sides of \eqref{eq:pre_rot} by their transposed matrices so that we can get rid of matrix $\cup$ as follows:
 %
 \begin{align}
 	U {(S+L)}^T(S+L) &= {(S+L)}^T{U}^TU(S+L) \nonumber \\
 	&= (S+L)(S+L) = {R}^TR.
 \end{align}
 %
 Now, we know that ${R}^TR$ is a symmetric positive definite matrix so that we can find the eigenvalues $\lambda_k$ and eigenvectors $\bm{v}_k$ using standard procedures \eg single value decomposition. Thus, since $S+L$ is symmetric and positive definite, it must have normalized eigenvectors, $\bm{v}_k$ and positive eigenvalues $\sqrt{\lambda_k}$  so that the Lagrange multipliers are
 %
 \begin{align}
 	l_{ij} = \Sigma_k \sqrt{\lambda_k}; \qquad \qquad \bm{v}_{ki} \bm{v}_{ki} - s_{ij}
 \end{align}
 %
 where $\bm{v}_{ki}$ signifies the $i$th component of $\bm{v}_k$ and the effect of the orthogonal matrix $U$ on these eigenvectors $\bm{a}_k$ is determined from \eqref{eq:pre_rot} which defines the unit vectors $\bm{q}_k$ as 
 %
 \begin{align}
 	\bm{q}_k = U . \bm{v}_k = \frac{1}{\sqrt{\lambda_k}} U (S+L) \bm{v}_k = \frac{1}{\sqrt{\lambda_k}} R \bm{v}_k.
 \end{align}
 
 The solution to find the constraint minimum of the minimum of the proposed cost function in \eqref{eq:kabsch_unconstraint} is then given by,
 %
 \begin{tcolorbox}[title=Kabsch's Optimal Rotation]
 	\begin{align}
 		u_{ij} = \Sigma_k b_{kl} a_{kj}.
 	\end{align}
 \end{tcolorbox}

\subsection{Examples}
%
There are clever ways of solving the optimal rotation between two vectors.

There is a jupyter notebook at the following  link: \href{https://colab.research.google.com/drive/1DL_Hq-Bp-pQuQyR6nDHnzwxIqhj4_Obq}{Kabsch Algorithm and Implementation}. For your convenience, it is included as a pdf file below.

\includepdf[pages=1-]{Kabsch.pdf}

\begin{homework}
	For the following model points $P$ and measured poinyts $Q$, compute the optimal rotation matrices for moving points $Q$ into point $P$. 
	%
	For the three assignments below, report your results within a colab notebook, download the colab notebook as a pdf and upload on Latte.
	\begin{enumerate}
		\item  \begin{align}
			P = \begin{bmatrix}
			-1 & 0 & 0 \\
			%
			0 & 0 & 0 \\
			%
			0 & 1 & 1
			\end{bmatrix},
			%
			\qquad 
			Q = \begin{bmatrix}
			0 & -1 & -1 \\
			0 & -1 & 0 \\
			0 & 0 & 0 \\
			-1 & 0 & 0
			\end{bmatrix}
		\end{align}
		%
		\item \begin{align}
			P = \begin{bmatrix}
			3172.79468418  &  727.52462347  & 7122.70450243 \\
			165.28953155 &  -3552.32467068 & -2045.15346584 \\
			5292.45250241 & -1748.52037006 & -6181.40300009 \\
			1893.07584225  & 5897.19719625  & 3130.41287776
			\end{bmatrix}, 
			\\
			Q = \begin{bmatrix}
			1774.11606309 & -4241.11341178  & 5259.04277742 \\
			6079.70499031  &  -98.14197972 & -3442.0914569 \\
			813.07069876  & 3334.26289147 & -6112.55652513 \\
			1856.72080823 &  2328.86927901  & 6322.16611888 \\
			\end{bmatrix} \nonumber
		\end{align}
		%
		\item For a toy problem, measure the coordinates of an object in the world using your favorite measuring instrument (a 3D camera sensor, iPhone app (e.g. ArkIt), android app e.t.c.). Be sure to record the position of the object at multiple points in world coordinates and make sure that the physical locations of these points are known (these are your model points). Then compute the optimal rotation and translation between the model and measured points.
	\end{enumerate}
\end{homework}

\section{Iterative Closest Point}
